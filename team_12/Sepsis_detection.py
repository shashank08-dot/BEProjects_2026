# -*- coding: utf-8 -*-
"""sepsis-detection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Oa6sOD8qAO6wnhV3SGaxLKyMW5JZUfKH
"""

!pip install wfdb
!pip install pandas numpy scikit-learn tensorflow keras

import os

print(len(os.listdir('/content/sample_data')))

!wget -r -N -c -np --cut-dirs=6 -nH https://physionet.org/files/challenge-2019/1.0.0/training/training_setA/

!ls /content | head -20

from google.colab import drive
drive.mount('/content/drive')

!mkdir -p /content/drive/MyDrive/sepsis_data

!mv /content/*.psv /content/drive/MyDrive/sepsis_data/

!ls /content/drive/MyDrive/sepsis_data | head -20

import pandas as pd

# Path to your file in Drive
file_path = '/content/drive/MyDrive/sepsis_data/p000001.psv'

# Read the PSV file
df = pd.read_csv(file_path, sep='|')

# Show the first 5 rows
df.head()

!ls /content/drive/MyDrive/sepsis_data | head -20

import pandas as pd
import os

# Path to your sepsis_data folder in Drive
folder_path = '/content/drive/MyDrive/sepsis_data'

# List of all .psv files in the folder
psv_files = [f for f in os.listdir(folder_path) if f.endswith('.psv')]

# Initialize an empty list to store individual dataframes
dfs = []

# Loop through files, read and append to dfs list
for file in psv_files:
    file_path = os.path.join(folder_path, file)
    df = pd.read_csv(file_path, sep='|')
    df['patient_id'] = file.replace('.psv', '')  # Optional: add patient_id column
    dfs.append(df)

# Concatenate all dataframes together
combined_df = pd.concat(dfs, ignore_index=True)

# Show combined dataframe shape and a sample
print(combined_df.shape)
combined_df.head()

# Check for missing values in each column
combined_df.isnull().sum()

# Forward fill missing values for each patient individually
combined_df.fillna(method='ffill', inplace=True)

# Then backward fill if any NaNs remain (at start of each patient data)
combined_df.fillna(method='bfill', inplace=True)

combined_df.isnull().sum()

from sklearn.preprocessing import MinMaxScaler

# Get a list of numeric columns (excluding patient_id, SepsisLabel)
numeric_cols = combined_df.columns.drop(['patient_id', 'SepsisLabel'])

# Initialize scaler
scaler = MinMaxScaler()

# Fit and transform the numeric columns
combined_df[numeric_cols] = scaler.fit_transform(combined_df[numeric_cols])

# Check result
combined_df.head()

from sklearn.model_selection import train_test_split

# Features (X) and Target (y)
X = combined_df.drop(['SepsisLabel', 'patient_id'], axis=1)
y = combined_df['SepsisLabel']

# Split into train-test sets (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report

# Initialize model
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)

# Train model
rf_model.fit(X_train, y_train)

# Predict on test set
y_pred = rf_model.predict(X_test)
# Save original dataset variables with unique names
X_train_orig, X_test_orig = X_train.copy(), X_test.copy()
y_train_orig, y_test_orig = y_train.copy(), y_test.copy()
y_pred_orig = y_pred.copy()

# Get probability predictions for ROC & PR curves
y_proba_orig = rf_model.predict_proba(X_test_orig)[:, 1]

# Accuracy
print("Accuracy:", accuracy_score(y_test, y_pred))

# Precision
print("Precision:", precision_score(y_test, y_pred))

# Recall
print("Recall:", recall_score(y_test, y_pred))

# F1 Score
print("F1 Score:", f1_score(y_test, y_pred))

# Confusion Matrix
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))

# Detailed Report
print("\nClassification Report:\n", classification_report(y_test, y_pred))

!pip install tensorflow pandas numpy scikit-learn

import os
import numpy as np
import pandas as pd
import random
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier

SEED = 42
os.environ['PYTHONHASHSEED'] = str(SEED)
random.seed(SEED)
np.random.seed(SEED)
tf.random.set_seed(SEED)

# Cell 1: setup
import os, random
import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score, confusion_matrix,
    roc_auc_score, roc_curve, precision_recall_curve, average_precision_score
)
import matplotlib.pyplot as plt
from google.colab import drive

# Mount Drive
drive.mount('/content/drive')

# Reproducibility
SEED = 42
np.random.seed(SEED)
random.seed(SEED)
tf.random.set_seed(SEED)

# Settings (fast & practical)
LATENT_DIM = 8
HIDDEN_DIMS = (64, 32)
VAE_MAX_EPOCHS = 60
BATCH_SIZE = 512
EARLYSTOP_PATIENCE = 5
MINORITY_MULTIPLIER = 2   # C: boost positives by 2x
OUT_DIR = '/content/drive/MyDrive'
os.makedirs(OUT_DIR, exist_ok=True)

print("Ready; OUT_DIR =", OUT_DIR)

# Cell 2: prepare data & scaler
assert 'combined_df' in globals(), "combined_df not found — load your combined dataframe first."

# Exclude patient_id and label
feature_cols = [c for c in combined_df.columns if c not in ('SepsisLabel', 'patient_id')]

X_all = combined_df[feature_cols].astype(np.float32).values
y_all = combined_df['SepsisLabel'].astype(int).values

# Stratified split
X_train, X_test, y_train, y_test = train_test_split(X_all, y_all, test_size=0.20, random_state=SEED, stratify=y_all)

print("Train shape:", X_train.shape, "Test shape:", X_test.shape)
print("Train class counts:", np.bincount(y_train))

# Fit MinMax on train and transform both
scaler = MinMaxScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Cell 3: build & train VAE on minority class only
minority = 1
X_minority = X_train_scaled[y_train == minority]
n_min = X_minority.shape[0]
n_maj = (y_train == 0).sum()
print("Minority (train):", n_min, "Majority (train):", n_maj)

# sampling layer
class Sampling(layers.Layer):
    def call(self, inputs):
        z_mean, z_log_var = inputs
        eps = tf.random.normal(shape=tf.shape(z_mean))
        return z_mean + tf.exp(0.5 * z_log_var) * eps

def build_small_vae(input_dim, latent_dim=LATENT_DIM, hidden_dims=HIDDEN_DIMS):
    # encoder
    inputs = keras.Input(shape=(input_dim,))
    x = inputs
    for h in hidden_dims:
        x = layers.Dense(h, activation='relu')(x)
    z_mean = layers.Dense(latent_dim, name='z_mean')(x)
    z_log_var = layers.Dense(latent_dim, name='z_log_var')(x)
    z = Sampling()([z_mean, z_log_var])
    encoder = keras.Model(inputs, [z_mean, z_log_var, z], name='encoder')

    # decoder
    latent_inputs = keras.Input(shape=(latent_dim,))
    x = latent_inputs
    for h in hidden_dims[::-1]:
        x = layers.Dense(h, activation='relu')(x)
    outputs = layers.Dense(input_dim, activation='sigmoid')(x)
    decoder = keras.Model(latent_inputs, outputs, name='decoder')

    # VAE subclass
    class VAE(keras.Model):
        def __init__(self, encoder, decoder, **kwargs):
            super().__init__(**kwargs)
            self.encoder = encoder
            self.decoder = decoder

        def train_step(self, data):
            if isinstance(data, tuple):
                data = data[0]
            with tf.GradientTape() as tape:
                z_mean, z_log_var, z = self.encoder(data)
                reconstruction = self.decoder(z)
                recon_loss = tf.reduce_mean(keras.losses.mse(data, reconstruction))
                kl_loss = -0.5 * tf.reduce_mean(1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))
                total_loss = recon_loss + 1e-3 * kl_loss
            grads = tape.gradient(total_loss, self.trainable_weights)
            self.optimizer.apply_gradients(zip(grads, self.trainable_weights))
            return {"loss": total_loss, "recon_loss": recon_loss, "kl_loss": kl_loss}

        def call(self, inputs):
            _, _, z = self.encoder(inputs)
            return self.decoder(z)

    vae = VAE(encoder, decoder)
    vae.compile(optimizer=keras.optimizers.Adam(1e-3))
    return vae, encoder, decoder

# Train VAE on minority
if n_min < 10:
    raise RuntimeError("Too few minority samples to train VAE.")
vae, enc, dec = build_small_vae(X_train_scaled.shape[1])
callbacks = [keras.callbacks.EarlyStopping(monitor='loss', patience=EARLYSTOP_PATIENCE, restore_best_weights=True)]
hist = vae.fit(X_minority, X_minority, epochs=VAE_MAX_EPOCHS, batch_size=BATCH_SIZE, callbacks=callbacks, verbose=1)
print("VAE trained. epochs run:", len(hist.history['loss']))

# Cell 4: generate synthetic set (balanced-ish) and save CSVs
# Decide counts
n_pos_gen = int(n_min * MINORITY_MULTIPLIER)          # generate 2x minority positives
n_neg_gen = n_pos_gen                                 # make synthetic dataset balanced

print("Generating positive synthetic samples:", n_pos_gen)
z = np.random.normal(size=(n_pos_gen, LATENT_DIM))
X_pos_synth_scaled = dec.predict(z, batch_size=1024)  # scaled in [0,1]

# For negatives: sample real negatives and add tiny gaussian noise
neg_pool = X_train_scaled[y_train == 0]
idx = np.random.choice(len(neg_pool), size=n_neg_gen, replace=True)
X_neg_sampled = neg_pool[idx]
noise = np.random.normal(loc=0.0, scale=0.01, size=X_neg_sampled.shape)
X_neg_synth_scaled = np.clip(X_neg_sampled + noise, 0.0, 1.0)

# Build synthetic scaled DF
synth_scaled_feats = np.vstack([X_pos_synth_scaled, X_neg_synth_scaled])
synth_labels = np.array([1]*n_pos_gen + [0]*n_neg_gen)
synthetic_df_scaled = pd.DataFrame(synth_scaled_feats, columns=feature_cols)
synthetic_df_scaled['SepsisLabel'] = synth_labels

# Inverse transform scaled -> original for saving
synth_orig_feats = scaler.inverse_transform(synthetic_df_scaled[feature_cols].values)
synthetic_df_orig = pd.DataFrame(synth_orig_feats, columns=feature_cols)
synthetic_df_orig['SepsisLabel'] = synthetic_df_scaled['SepsisLabel'].values

# Real train df (original scale)
real_orig_feats = scaler.inverse_transform(X_train_scaled)
real_df_orig = pd.DataFrame(real_orig_feats, columns=feature_cols)
real_df_orig['SepsisLabel'] = y_train

# Combined saved df (original scale)
combined_saved_df = pd.concat([real_df_orig, synthetic_df_orig], ignore_index=True)

# Save CSVs (will overwrite)
real_path  = os.path.join(OUT_DIR, 'sepsis_real_original_scale.csv')
synth_path = os.path.join(OUT_DIR, 'sepsis_synthetic_original_scale.csv')
comb_path  = os.path.join(OUT_DIR, 'sepsis_combined_original_scale.csv')

real_df_orig.to_csv(real_path, index=False, float_format='%.6f')
synthetic_df_orig.to_csv(synth_path, index=False, float_format='%.6f')
combined_saved_df.to_csv(comb_path, index=False, float_format='%.6f')

print("Saved CSVs:")
print(real_path)
print(synth_path)
print(comb_path)
print("Synthetic class counts:", synthetic_df_orig['SepsisLabel'].value_counts().to_dict())

# Cell 5: RF train & evaluate on held-out X_test_scaled / y_test
# Prepare training arrays (scaled)
X_tr_real = X_train_scaled
y_tr_real = y_train.astype(int)

X_tr_synth = synthetic_df_scaled[feature_cols].values
y_tr_synth = synthetic_df_scaled['SepsisLabel'].values.astype(int)

X_tr_comb = np.vstack([X_tr_real, X_tr_synth])
y_tr_comb = np.concatenate([y_tr_real, y_tr_synth])

print("Training sizes -> real:", X_tr_real.shape, "synth:", X_tr_synth.shape, "combined:", X_tr_comb.shape)

rf_params = {"n_estimators":200, "random_state":SEED, "n_jobs":-1}

def train_eval(X_tr, y_tr, X_te, y_te, name):
    clf = RandomForestClassifier(**rf_params)
    clf.fit(X_tr, y_tr)
    y_pred = clf.predict(X_te)
    # handle case where model learned single class (unlikely now)
    proba = clf.predict_proba(X_te)
    if proba.shape[1] == 1:
        y_proba = np.zeros(len(y_te)) if clf.classes_[0] == 0 else np.ones(len(y_te))
    else:
        # probability for positive class (class label == 1)
        pos_idx = list(clf.classes_).index(1)
        y_proba = proba[:, pos_idx]
    metrics = {
        "Accuracy": accuracy_score(y_te, y_pred),
        "Precision": precision_score(y_te, y_pred, zero_division=0),
        "Recall": recall_score(y_te, y_pred, zero_division=0),
        "F1": f1_score(y_te, y_pred, zero_division=0),
        "Confusion": confusion_matrix(y_te, y_pred),
        "y_proba": y_proba,
        "clf": clf
    }
    print(f"\n--- {name} ---")
    print("Accuracy: {:.4f}  Precision: {:.4f}  Recall: {:.4f}  F1: {:.4f}".format(
        metrics["Accuracy"], metrics["Precision"], metrics["Recall"], metrics["F1"]))
    print("Confusion Matrix:\n", metrics["Confusion"])
    return metrics

res_real  = train_eval(X_tr_real, y_tr_real, X_test_scaled, y_test, "REAL-only")
res_synth = train_eval(X_tr_synth, y_tr_synth, X_test_scaled, y_test, "SYNTHETIC-only")
res_comb  = train_eval(X_tr_comb, y_tr_comb, X_test_scaled, y_test, "COMBINED (real+synthetic)")

# Summary table
summary = pd.DataFrame([
    {"Dataset":"REAL-only", **{k: v for k,v in res_real.items() if k in ['Accuracy','Precision','Recall','F1']}},
    {"Dataset":"SYNTHETIC-only", **{k: v for k,v in res_synth.items() if k in ['Accuracy','Precision','Recall','F1']}},
    {"Dataset":"COMBINED", **{k: v for k,v in res_comb.items() if k in ['Accuracy','Precision','Recall','F1']}},
])
print("\nSummary:\n", summary[['Dataset','Accuracy','Precision','Recall','F1']])

# ROC + PR plots (zoomed)
y_proba_real = res_real['y_proba']
y_proba_synth = res_synth['y_proba']
y_proba_comb = res_comb['y_proba']

from sklearn.metrics import roc_auc_score, roc_curve, precision_recall_curve, average_precision_score

fpr_r, tpr_r, _ = roc_curve(y_test, y_proba_real)
fpr_s, tpr_s, _ = roc_curve(y_test, y_proba_synth)
fpr_c, tpr_c, _ = roc_curve(y_test, y_proba_comb)
auc_r = roc_auc_score(y_test, y_proba_real)
auc_s = roc_auc_score(y_test, y_proba_synth)
auc_c = roc_auc_score(y_test, y_proba_comb)

plt.figure(figsize=(12,5))
plt.subplot(1,2,1)
plt.plot(fpr_r, tpr_r, '-', linewidth=2, label=f'Real (AUC={auc_r:.3f})')
plt.plot(fpr_s, tpr_s, '--', linewidth=2, label=f'Synth (AUC={auc_s:.3f})')
plt.plot(fpr_c, tpr_c, ':', linewidth=2, label=f'Combined (AUC={auc_c:.3f})')
plt.plot([0,1],[0,1],'k--', alpha=0.3)
plt.xlim([0.0, 0.2]); plt.ylim([0.8, 1.01])
plt.xlabel('False Positive Rate'); plt.ylabel('True Positive Rate'); plt.title('ROC (zoomed)')
plt.legend(); plt.grid(alpha=0.3)

plt.subplot(1,2,2)
prec_r, rec_r, _ = precision_recall_curve(y_test, y_proba_real)
prec_s, rec_s, _ = precision_recall_curve(y_test, y_proba_synth)
prec_c, rec_c, _ = precision_recall_curve(y_test, y_proba_comb)
ap_r = average_precision_score(y_test, y_proba_real)
ap_s = average_precision_score(y_test, y_proba_synth)
ap_c = average_precision_score(y_test, y_proba_comb)

plt.plot(rec_r, prec_r, '-', linewidth=2, label=f'Real (AP={ap_r:.3f})')
plt.plot(rec_s, prec_s, '--', linewidth=2, label=f'Synth (AP={ap_s:.3f})')
plt.plot(rec_c, prec_c, ':', linewidth=2, label=f'Combined (AP={ap_c:.3f})')
plt.xlabel('Recall'); plt.ylabel('Precision'); plt.title('Precision-Recall')
plt.legend(); plt.grid(alpha=0.3)
plt.tight_layout()

plot_path = os.path.join(OUT_DIR, 'sepsis_models_comparison_fixed.png')
plt.savefig(plot_path, bbox_inches='tight', dpi=150)
print("Saved plot:", plot_path)
plt.show()

import joblib

model_path = "/content/drive/MyDrive/sepsis_models/combined/model.pkl"

model = joblib.load(model_path)

try:
    print(model.feature_names_in_)
except:
    print("Model does not store feature names. Using dataset column order instead.")

plt.figure(figsize=(12,5))

# --- ROC PLOT (full view) ---
plt.subplot(1,2,1)
plt.plot(fpr_r, tpr_r, label=f'Real (AUC={auc_r:.3f})', linewidth=3, color='yellow')
plt.plot(fpr_s, tpr_s, label=f'Synthetic (AUC={auc_s:.3f})', linewidth=3,
         color='red', linestyle='--')
plt.plot(fpr_c, tpr_c, label=f'Combined (AUC={auc_c:.3f})', linewidth=3,
         color='green', linestyle=':')
plt.plot([0,1],[0,1],'k--', alpha=0.3)

plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve (Full)')
plt.legend()
plt.grid(alpha=0.3)

# --- PRECISION-RECALL PLOT ---
plt.subplot(1,2,2)
plt.plot(rec_r, prec_r, label=f'Real (AP={ap_r:.3f})', linewidth=3, color='yellow')
plt.plot(rec_s, prec_s, label=f'Synthetic (AP={ap_s:.3f})', linewidth=3,
         color='red', linestyle='--')
plt.plot(rec_c, prec_c, label=f'Combined (AP={ap_c:.3f})', linewidth=3,
         color='green', linestyle=':')

plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision-Recall Curve')
plt.legend()
plt.grid(alpha=0.3)

plt.tight_layout()
plt.show()

plt.figure(figsize=(12,5))

# --- ROC PLOT (full view) ---
plt.subplot(1,2,1)
plt.plot(fpr_r, tpr_r, label=f'Real (AUC={auc_r:.3f})', linewidth=3, color='yellow')
plt.plot(fpr_s, tpr_s, label=f'Synthetic (AUC={auc_s:.3f})', linewidth=3,
         color='red', linestyle='--')
plt.plot(fpr_c, tpr_c, label=f'Combined (AUC={auc_c:.3f})', linewidth=3,
         color='green', linestyle=':')
plt.plot([0,1],[0,1],'k--', alpha=0.3)

plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve (Full)')
plt.legend()
plt.grid(alpha=0.3)

# --- PRECISION-RECALL PLOT ---
plt.subplot(1,2,2)
plt.plot(rec_r, prec_r, label=f'Real (AP={ap_r:.3f})', linewidth=3, color='yellow')
plt.plot(rec_s, prec_s, label=f'Synthetic (AP={ap_s:.3f})', linewidth=3,
         color='red', linestyle='--')
plt.plot(rec_c, prec_c, label=f'Combined (AP={ap_c:.3f})', linewidth=3,
         color='green', linestyle=':')

plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision-Recall Curve')
plt.legend()
plt.grid(alpha=0.3)

plt.tight_layout()
plt.show()

import joblib
import os

# Create directory structure
base_dir = "/content/drive/MyDrive/sepsis_models"
paths = {
    "real": os.path.join(base_dir, "real"),
    "synthetic": os.path.join(base_dir, "synthetic"),
    "combined": os.path.join(base_dir, "combined")
}

for p in paths.values():
    os.makedirs(p, exist_ok=True)

# ---------------- SAVE REAL MODEL ----------------
joblib.dump(res_real["clf"], os.path.join(paths["real"], "model.pkl"))
joblib.dump(scaler, os.path.join(paths["real"], "scaler.pkl"))

# ---------------- SAVE SYNTHETIC MODEL ----------------
joblib.dump(res_synth["clf"], os.path.join(paths["synthetic"], "model.pkl"))
joblib.dump(scaler, os.path.join(paths["synthetic"], "scaler.pkl"))

# ---------------- SAVE COMBINED MODEL ----------------
joblib.dump(res_comb["clf"], os.path.join(paths["combined"], "model.pkl"))
joblib.dump(scaler, os.path.join(paths["combined"], "scaler.pkl"))

print("✅ All three models saved successfully at:", base_dir)

import os

for root, dirs, files in os.walk("/content/drive/MyDrive"):
    for file in files:
        if "scaler" in file.lower():
            print(os.path.join(root, file))

# ---- BAR CHART FOR METRICS (Accuracy, Precision, Recall, F1, ROC-AUC) ----

# Compute ROC-AUC values from earlier results
auc_real = roc_auc_score(y_test, res_real["y_proba"])
auc_synth = roc_auc_score(y_test, res_synth["y_proba"])
auc_comb = roc_auc_score(y_test, res_comb["y_proba"])

# Create performance table
performance_data = pd.DataFrame({
    "Dataset": ["Real", "Synthetic", "Combined"],
    "Accuracy": [res_real["Accuracy"], res_synth["Accuracy"], res_comb["Accuracy"]],
    "Precision": [res_real["Precision"], res_synth["Precision"], res_comb["Precision"]],
    "Recall": [res_real["Recall"], res_synth["Recall"], res_comb["Recall"]],
    "F1": [res_real["F1"], res_synth["F1"], res_comb["F1"]],
    "ROC-AUC": [auc_real, auc_synth, auc_comb]
})

# Plot bar graph
plt.figure(figsize=(12,6))
performance_data.set_index("Dataset").plot(kind="bar", figsize=(12,6), grid=True)

plt.title("Model Performance Comparison")
plt.ylabel("Score")
plt.ylim(0, 1.05)
plt.legend(loc="lower left", bbox_to_anchor=(1, 0.5))
plt.tight_layout()

plot_bar_path = os.path.join(OUT_DIR, "sepsis_performance_barchart.png")
plt.savefig(plot_bar_path, dpi=150, bbox_inches="tight")
plt.show()

print("Bar graph saved to:", plot_bar_path)



import numpy as np
import matplotlib.pyplot as plt

# ------------------------------------------------------
# Assumed metric values (you can replace with your own)
# ------------------------------------------------------

metrics = [
    "Accuracy", "Precision", "Recall\n(Sensitivity)", "Specificity",
    "F1-Score", "Balanced\nAccuracy", "ROC-AUC", "PR-AUC"
]

# Example realistic values
original_scores = [0.91, 0.89, 0.88, 0.93, 0.88, 0.90, 0.913, 0.842]
synthetic_scores = [0.89, 0.87, 0.86, 0.90, 0.86, 0.88, 0.895, 0.811]

# ------------------------------------------------------
# Bar Chart Visualization
# ------------------------------------------------------

x = np.arange(len(metrics))
width = 0.35  # width of bars

plt.figure(figsize=(12, 6))

# Bars
plt.bar(x - width/2, original_scores, width, label="Original Data", color="salmon")
plt.bar(x + width/2, synthetic_scores, width, label="Synthetic Data", color="skyblue")

# Title & labels
plt.ylabel("Score", fontsize=12)
plt.title("Performance Metrics Comparison: Original vs Synthetic Sepsis Models",
          fontsize=14)
plt.xticks(x, metrics, rotation=45, ha="right")
plt.ylim(0, 1.10)
plt.grid(axis="y", linestyle="--", alpha=0.6)

plt.legend(fontsize=12)
plt.tight_layout()
plt.show()